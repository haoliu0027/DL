{"nbformat":4,"nbformat_minor":0,"metadata":{"gist":{"data":{"description":"/nbs/blog_test.ipynb","public":true},"id":""},"jupytext":{"split_at_heading":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"Blog_Group56.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"zIO7VSq_M4M9","colab_type":"text"},"source":["# Paper Reproduction - \"Image-to-Image Translation with Conditional Adversarial Networks\"\n","\n","> By $\\;\\;\\;$   Xu Yingfu           $\\;\\;\\;\\;\\;$            Zeng Liang              $\\;\\;\\;\\;\\;$            Liu Hao"]},{"cell_type":"markdown","metadata":{"id":"Jn-n94FWM4M_","colab_type":"text"},"source":["## 1. Introduction"]},{"cell_type":"markdown","metadata":{"id":"roUU3HIrM4M_","colab_type":"text"},"source":["Many problems in image processing, computer graphics and computer vision can be regarded as a problem that translate an input image to a new image with different style or format. Generative Adversarial Networks(GAN) is a of paramount method to implement the image translating. However, how effective image-conditional GANs can be as a general-purpose solition for image-to-image translation remains unclear. In this case, conditional GANs has been proposed in this paper [1] (https://arxiv.org/pdf/1611.07004.pdf) which is used to explore the effectness of image-condition GANs. And in this paper, it achives decent results on a wide variety of applications('Labels to Street Scene', 'Labels to Facade', 'BW to Color', 'Aerial to Map', 'Day to Night' and 'Edges to Photo'). For simplicity of expression, the method in [1] is called as \"pix2pix\" in the rest part of this report.\n","\n","By inspired by this paper, we would like to reproduce this paper to gain a deeper insight by following three critria: \n","\n","- New code variant (Rewrite existing code to be more efficient/readable)\n","- New data (Evaluating different datasets to obtain similar result)\n","- Hyperparamter check (Evaluating sensitivity to hyperparameters)\n","\n","In this blog, we will walk through the following:\n","\n"," - section 2 Code Description for New code variant: we only illustrate the significant changed part of code \n"," - section 3 New dataset: what dataset we choose and why choose this dataset\n"," - section 4 The results of the New dataset\n"," - section 5 Hyperparameter check: Due to limited time, we only choose \n"," - section 6 Appendix"]},{"cell_type":"markdown","metadata":{"id":"vs2bT2UDM4NA","colab_type":"text"},"source":["## 2. Code Description for New code variant"]},{"cell_type":"markdown","metadata":{"id":"6T3PyS_nM4NB","colab_type":"text"},"source":["In this section, we rewrite the existing code from the readability and the efficiency two aspects.\n","\n","- Readability: In the [orginal code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) of paper [1], it mixes of \"pix2pix\" model code and \"CycleGAN\" model code which is quite difficult for readers to follow its logic. For example, the `Resnet` architecture is redundant in the \"pix2pix\" model.  In this case, we pick up all \"pix2pix\" code and rewrite the code into modularity (dataset, dataloader, libraries, parameters, network, train, test)to give a clear logic.\n","\n","\n","- Efficiency: In the [original code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) of paper [1], the dataset format is troublesome since the dataset we found are usually separate pictures and the picture size is unique. In this case, much time spents on changing picture size and combine pictures by `combine_A_and_B.py`. Inspired by the [github code](https://github.com/mrzhu-cool/pix2pix-pytorch), we change the data format like the following. We don not need to change the data size and preprocess the dataset by `combine_A_and_B.py`.  \n","                           1st class folder:        - facades \n","                           2nd class folder:      - test  - train\n","                           3rd class folder:        - a     - a\n","                           3rd class folder:        - b     - b\n","                                \n","                                   \n","In the Appendix, the detailed explanation of the code will be inllustrated.\n","\n","Besides, we noticed that there are several differences between the code and the description in the paper[1].\n","\n","After the last layer, a convolution is applied to map to a 1-dimensional output, followed by a Sigmoid function. But in the code: Note: Do not use sigmoid as the last layer of Discriminator. LSGAN needs no sigmoid.\n","\n","CycleGAN: We use 6 blocks for 128 × 128 images and 9 blocks for 256 × 256 and higher-resolution training images.\n","for L GAN (Equation 1), we replace the negative log likelihood objective by a least-squares loss. This loss is more stable during training and generates higher quality results.\n","Pix2Pix: Previous approaches have found it beneficial to mix the GAN objective with a more traditional loss, such as L2 distance.\n","\n","\\begin{aligned}\n","\\mathcal{L}_{\\mathrm{GAN}}\\left(G, D_{Y}, X, Y\\right) &=\\mathbb{E}_{y \\sim p_{\\text {data}}(y)}\\left[\\log D_{Y}(y)\\right] \\\\\n","&+\\mathbb{E}_{x \\sim p_{\\text {data}}(x)}\\left[\\log \\left(1-D_{Y}(G(x))\\right]\\right.\n","\\end{aligned}\n","\n","changed the image buffer in Gan lose in both code. Fixed the Gan lose to least square loss. For the purpose of code understandeing and programing practice.\n","\n","\n","Pix2Pix: The 70 × 70 discriminator architecture is: C64-C128-C256-C512 \n","https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ka42RqdGM4NB","colab_type":"text"},"source":["## 3. New dataset\n","pix2pix model has achieved impressive results in many image-to-image translation applications. In order to further explored the ability of pix2pix,  we have designed 3 new application scenarios including the recovery of blurry images, Chinese calligraphy transformation, and turning depth images into RGB images. 3 new datasets were created for these applications respectively. In this section, the dataset creation process is introduced.\n","\n","\n","###Blurry images\n","It is difficult to collect paired blurry and clear images so we had to take advantage of the existing image dataset. We applied multiple blurring to the images from PASCAL Visual Object Classes Challenge 2007, which consists of 2501 training images, 2510 validation images and 4952 test images regarding 20 common objects. For each image in the original dataset, 9 blurry images were created by gaussian blur, motion blur, and disk blur with random filter size and intensity as the example below. The new images are paired with the original one. The original partition is maintained. Thus, the new dataset has 22509 training images and 22590 validation images and 44568 test images. \n","\n","Method |  - |  -  \n","---|---|---\n","Gaussian blur| size in [3,15] | std in [1,5] |\n","Motion blur | length in [8, 20] | angle in [0 360]|\n","Disk blur | radius in [2,10] |  |\n","\n","<img src=\"http://i1.fuimg.com/716198/8059efe456298d29.png\" width = \"750\" height = \"300\" alt=\"font\" align=center />\n","\n","\n","###Chinese calligraphy\n","The Chinese calligraphy dataset is for training a model to transform characters of printing style to calligraphy. The images were generated from calligraphy fonts and those images of the same characters were made into pair. The dataset consists of 300 paired images for training, 100 for validation, and 100 for test. \n","\n","<img src=\"http://i1.fuimg.com/716198/38e43fb808f1a9b7.png\" width = \"600\" height = \"300\" alt=\"font\" align=center />\n","\n","\n","### Depth dataset\n","\n","The depth grayscale images and RGB scene images are sampled from the SYNTHIA-AL (ICCV Workshops 2019) dataset downloaded from http://synthia-dataset.net/downloads/. SYNTHIA is a dataset that has been generated with the purpose of aiding semantic segmentation and related scene understanding problems in the context of driving scenarios. SYNTHIA consists of a collection of photo-realistic frames rendered from a virtual city and comes with precise pixel-level semantic annotations [1]. The accurate depth of frames is also provided by the dataset. Depth is encoded in the 3 channels using the following formula: Depth = 5000 * (R + G*256 + B*256*256) / (256*256*256 - 1). We process the RGB images where depth is encoded, to get grayscale images with 1 channel. The intensity of each pixel is determined by the depth in meters. So the maximum depth encoded in grayscale images is 255 meters. We choose 3400 image pairs as the training set and 1000 pairs as the testing set. The cGAN is trained to generate the RGB scene image from the input grayscale depth image. The hyperparameters are the defaulted ones.\n","\n","optimizer = adam\n","beta1 = 0.5, beta2 = 0.999 momentum term of adam\n","lr = 0.0002 initial learning rate for adam\n","batch_size = 1 input batch size\n","n_epochs = 100 number of epochs with the initial learning rate \n","n_epochs_decay = 100 number of epochs to linearly decay learning rate to zero \n","activation function = ReLU ReLUs in the generator encoder and discriminator are leaky, with slope 0.2. ReLUs in the generator decoder are not leaky.\n","generator dropout rate = 50%\n"]},{"cell_type":"markdown","metadata":{"id":"sPW8ZnEMM4NC","colab_type":"text"},"source":["## 4. The results of the New dataset\n","分析loss的变化趋势，为什么不收敛\n","http://openaccess.thecvf.com/content_cvpr_2018/papers/Kupyn_DeblurGAN_Blind_Motion_CVPR_2018_paper.pdf\n","\n","https://arxiv.org/pdf/1612.02177.pdf\n","\n","https://blog.csdn.net/purgle/article/details/73719101\n","\n","4.1 The results of the deblurring dataset\n","\n","In the following figures, the deblurring results of the default pix2pix project on a small test set are shown. The default pix2pix uses unet_256 as the architecture of the generator network. The learning rate is 0.0002 and decay linearly to zero in 10 epochs after it holds 0.0002 for 10 epochs.\n","\n","This small test set is made up of 9 different kinds of artificial blur of the same image.\n","\n","Original sharp image:\n","\n","![](https://drive.google.com/uc?id=1k7KoeUVKoEvdjXLCXlCcY6iV263vExw-)\n","\n","Original blurry images:\n","![](https://drive.google.com/uc?id=1pfuIAVYM88HR6RQVaQys1Vl-E5XvIsO8)\n","Results after 1 epoch of training:\n","![](https://drive.google.com/uc?id=1gE7F7vPZF-nYiSkNvz0oCBg0akUNy0PA)\n","Results after 5 epochs of training:\n","![](https://drive.google.com/uc?id=1pWR_hU8JEP6rdNo8yqap22csQ5UL42n_)\n","Results after 10 epochs of training:\n","![](https://drive.google.com/uc?id=1fGNydEC7uuIawYpzPPNZIZOFWgF82Mfo)\n","Results after 15 epochs of training:\n","![](https://drive.google.com/uc?id=1VGDDWbb39FyI1KyQKmNK5lt2XA_y-zte)\n","Results after 20 epochs of training:\n","![](https://drive.google.com/uc?id=1yLnIw1qJg1FNFq62_LUhvCxbsGwJyUmb)\n","\n","4.2 The results of the depth dataset\n","\n","The results are shown in the \"Comparison on Different Network Architectures and Learning Rate\" part.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zWaK3gkrlCNS"},"source":["## 5. Comparison on Different Network Architectures and Learning Rate\n","5.1 Deblurring\n","\n","In the following figures, the deblurring results of 4 different models on a new small test set are shown. This test set is made up of 9 different kinds of artificial blur of the same image. The first three images from left have gaussian blur. The three images in the middle have motion blur. The three images on the right have disk blur.\n","\n","Shape image:\n","\n","![](https://drive.google.com/uc?id=1pS5NMre_HY7io82TQwMPS_7CgMwRzNDu)\n","\n","Original blurry images:\n","![](https://drive.google.com/uc?id=18TaTYeIaFYz9wY7jQdVZr35O4lmLH_NP)\n","Results of unet 0.0002:\n","![](https://drive.google.com/uc?id=1UsVjKh8DWecGY31mffn1pmreFT82SgiF)\n","Results of unet 0.0005:\n","![](https://drive.google.com/uc?id=1V8Ozpb2L84Ng9njzvCdIwiihp8CKAZ9T)\n","Results of resnet 0.0002:\n","![](https://drive.google.com/uc?id=1qkwxfRW1kqV1dog-9WB-qoiQ7ai2A9e6)\n","\n","In the following figures, the deblurring results of 4 different models on a new small test set are shown. This test set is made up by originally blurry images found on the internet.\n","\n","Original blurry images:\n","![](https://drive.google.com/uc?id=1Z8ZOnFU8o581BlVptuK7kOO5KQ_5UjqC)\n","Results of unet 0.0002:\n","![](https://drive.google.com/uc?id=1ME8oQy-wY3US3o93zStsh_VssKTLelB0)\n","Results of unet 0.0005:\n","![](https://drive.google.com/uc?id=1gT5fF4gSyzyDo7GHJJdSTNPDWiwoLQXU)\n","Results of resnet 0.0002:\n","![](https://drive.google.com/uc?id=1tCW6sDrPLK8BPTxPEhKTgAM_0J5Kuo7E)\n","\n","5.2 Generating RGB images from graysacle depth images\n","\n","In the following figures, the generated RGB images from input graysacle depth images of 2 different models on a small test set are shown. This test set is selected from the visualization dataset of depth dataset. Each image in this set has a unique scene in the simulatied city. These 7 kinds of scene are also included in the traning set.\n","\n","Original RGB images:\n","![](https://drive.google.com/uc?id=1rTkwjIaE9HjcvsWceZLsvx8XKM80RFM1)\n","Results of unet 0.0002:\n","![](https://drive.google.com/uc?id=1zVhPX1P6rFW-HzydimvvnsufrRlDwVod)\n","Results of resnet 0.0002:\n","![](https://drive.google.com/uc?id=1sl7ESSp6cyTvp87Zm0JdgOMjedhUQhGF)\n","\n","In the following figures, the generated RGB images of 2 different models on a new small test set are shown. This test set is selected from the SYNTHIA-SF-19 dataset. Each image in this set has a unique scene in the simulatied city. These 7 kinds of scene are NOT included in the traning set.\n","Input grayscale depth images:\n","![](https://drive.google.com/uc?id=1Z6lAK-Ym6Oqv8jMBwrHLSdAd4A13yxz1)\n","Original RGB images:\n","![](https://drive.google.com/uc?id=1LC9jjKXXAEN455qDDp_8eVDhsUNTmN0H)\n","Results of unet 0.0002:\n","![](https://drive.google.com/uc?id=1u4YyVMUTESm8iWPc84CPZLRIAFMIVGW2)\n","Results of resnet 0.0002:\n","![](https://drive.google.com/uc?id=1UXBqbh1ADItTLyNFkPOhKHThhIZivhgn)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NJbrNX4uM4ND","colab_type":"text"},"source":["## 7. Appendix\n","In the below, we provide two possible ways to run our code to see the result. \n","\n","- 1: you have to connect the colab to the google drive for accessing the datasets.\n","- 2: you direct download the github dataset folders(recommended)\n","\n","\n","    \n","\n"]},{"cell_type":"markdown","metadata":{"id":"2pP8pH2Gx5ZN","colab_type":"text"},"source":["### **Github clone:**\n"]},{"cell_type":"code","metadata":{"id":"9NaO-pTWyEhA","colab_type":"code","colab":{}},"source":["!git clone https://github.com/YingfuXu/pix2pixCourseProject"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2p9jJy4MM4ND","colab_type":"text"},"source":["### **Library used**"]},{"cell_type":"code","metadata":{"id":"-tv-3-DMM4NE","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import os\n","from os import listdir\n","from os.path import join\n","from math import log10\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import init\n","from torch.optim import lr_scheduler\n","from torch.utils.data import DataLoader\n","import torch.backends.cudnn as cudnn\n","import functools\n","from matplotlib import pyplot as plt\n","import torch.utils.data as data\n","import numpy as np\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fT465B8tM4NH","colab_type":"text"},"source":["### **setting parameters**"]},{"cell_type":"code","metadata":{"id":"18JRgF3-M4NH","colab_type":"code","colab":{}},"source":["# change the dataset in the current directory\n","dataset = 'facades'\n","\n","batch_size = 1\n","test_batch_size=1\n","# direction of the dataset\n","direction='b2a'\n","# number of channels\n","input_nc=3\n","output_nc=3\n","# the number of filters in the first convolution layer\n","ngf=64\n","ndf=64\n","\n","epoch_count=1\n","niter=100\n","niter_decay=100\n","\n","lr=0.0002\n","lr_policy='lambda'\n","lr_decay_iters=30\n","beta1=0.5\n","\n","threads=0\n","seed=123\n","lamb=10\n","\n","torch.manual_seed(seed)\n","# this is the cpu version\n","# device = torch.device(\"cpu\")\n","\n","# if using the gpu, open the following code\n","torch.cuda.manual_seed(seed)\n","device = torch.device(\"cuda: 0\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gaOe70OYM4NK","colab_type":"text"},"source":["### **Funtions for loading dataset and preprocessing**"]},{"cell_type":"code","metadata":{"id":"kn6RkQRlM4NK","colab_type":"code","colab":{}},"source":["def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n","\n","def load_img(filepath):\n","    img = Image.open(filepath).convert('RGB')\n","    img = img.resize((256, 256), Image.BICUBIC)\n","    return img\n","\n","def save_img(image_tensor, filename):\n","    image_numpy = image_tensor.float().numpy()\n","    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n","    image_numpy = image_numpy.clip(0, 255)\n","    image_numpy = image_numpy.astype(np.uint8)\n","    image_pil = Image.fromarray(image_numpy)\n","    image_pil.save(filename)\n","    print(\"Image saved as {}\".format(filename))\n","\n","\n","# Inherit the data.Dataset and create a new dataset for getting each item easily\n","class DatasetFromFolder(data.Dataset):\n","    \n","    def __init__(self, image_dir, direction):\n","        super(DatasetFromFolder, self).__init__()\n","        self.direction = direction\n","        self.a_path = join(image_dir, \"a\")\n","        self.b_path = join(image_dir, \"b\")\n","        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\n","\n","        transform_list = [transforms.ToTensor(),\n","                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n","        self.transform = transforms.Compose(transform_list)\n","        \n","    def __getitem__(self, index):\n","        # covert into RGB picture, do they cover the original pictures?\n","        # Answer: this is just converting the original pictures \n","        # join : combine each paths in the list \n","        # Preprocessing of the pictures\n","        a = Image.open(join(self.a_path, self.image_filenames[index])).convert('RGB')\n","        b = Image.open(join(self.b_path, self.image_filenames[index])).convert('RGB')\n","        # Resize\n","        a = a.resize((286, 286), Image.BICUBIC)\n","        b = b.resize((286, 286), Image.BICUBIC)\n","        # To tensor\n","        a = transforms.ToTensor()(a)\n","        b = transforms.ToTensor()(b)\n","        # add a offset to the picture\n","        w_offset = random.randint(0, max(0, 286 - 256 - 1))\n","        h_offset = random.randint(0, max(0, 286 - 256 - 1))\n","    \n","        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n","        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n","        # Normalize\n","        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n","        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n","        \n","      \n","        if random.random() < 0.5:\n","            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n","            idx = torch.LongTensor(idx)\n","            a = a.index_select(2, idx)\n","            b = b.index_select(2, idx)\n","\n","        if self.direction == \"a2b\":\n","            return a, b\n","        else:\n","            return b, a\n","\n","    def __len__(self):\n","        return len(self.image_filenames)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJRMrts4M4NN","colab_type":"text"},"source":["### **Design Network**"]},{"cell_type":"code","metadata":{"id":"3DVRb_j8M4NO","colab_type":"code","colab":{}},"source":["class Inconv(nn.Module):\n","    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\n","        super(Inconv, self).__init__()\n","        self.inconv = nn.Sequential(\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0,\n","                      bias=use_bias),\n","            norm_layer(out_ch),\n","            nn.ReLU(True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.inconv(x)\n","        return x\n","# ngf, ngf * 2, norm_layer, use_bias\n","class Down(nn.Module):\n","    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\n","        super(Down, self).__init__()\n","        self.down = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, kernel_size=3,\n","                      stride=2, padding=1, bias=use_bias),\n","            norm_layer(out_ch),\n","            nn.ReLU(True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.down(x)\n","        return x\n","\n","class Up(nn.Module):\n","    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\n","        super(Up, self).__init__()\n","        self.up = nn.Sequential(\n","            # nn.Upsample(scale_factor=2, mode='nearest'),\n","            # nn.Conv2d(in_ch, out_ch,\n","            #           kernel_size=3, stride=1,\n","            #           padding=1, bias=use_bias),\n","            nn.ConvTranspose2d(in_ch, out_ch,\n","                               kernel_size=3, stride=2,\n","                               padding=1, output_padding=1,\n","                               bias=use_bias),\n","            norm_layer(out_ch),\n","            nn.ReLU(True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.up(x)\n","        return x\n","\n","\n","class Outconv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(Outconv, self).__init__()\n","        self.outconv = nn.Sequential(\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.outconv(x)\n","        return x\n","\n","def init_net(net, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\n","    \n","    with torch.no_grad():\n","        net.to(gpu_id)\n","        init_weights(net, init_type, gain=init_gain)\n","    return net"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1QcUceMM4NR","colab_type":"text"},"source":["### **gernerator**"]},{"cell_type":"code","metadata":{"id":"b7uYCsztM4NS","colab_type":"code","colab":{}},"source":["class UnetSkipConnectionBlock(nn.Module):\n","    \"\"\"Defines the Unet submodule with skip connection.\n","        X -------------------identity----------------------\n","        |-- downsampling -- |submodule| -- upsampling --|\n","    \"\"\"\n","\n","    def __init__(self, outer_nc, inner_nc, input_nc=None,\n","                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n","        \"\"\"Construct a Unet submodule with skip connections.\n","\n","        Parameters:\n","            outer_nc (int) -- the number of filters in the outer conv layer\n","            inner_nc (int) -- the number of filters in the inner conv layer\n","            input_nc (int) -- the number of channels in input images/features\n","            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n","            outermost (bool)    -- if this module is the outermost module\n","            innermost (bool)    -- if this module is the innermost module\n","            norm_layer          -- normalization layer\n","            use_dropout (bool)  -- if use dropout layers.\n","        \"\"\"\n","        super(UnetSkipConnectionBlock, self).__init__()\n","        self.outermost = outermost\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","        if input_nc is None:\n","            input_nc = outer_nc\n","        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n","                             stride=2, padding=1, bias=use_bias)\n","        downrelu = nn.LeakyReLU(0.2, True)\n","        downnorm = norm_layer(inner_nc)\n","        uprelu = nn.ReLU(True)\n","        upnorm = norm_layer(outer_nc)\n","\n","        if outermost:\n","            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n","                                        kernel_size=4, stride=2,\n","                                        padding=1)\n","            down = [downconv]\n","            up = [uprelu, upconv, nn.Tanh()]\n","            model = down + [submodule] + up\n","        elif innermost:\n","            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n","                                        kernel_size=4, stride=2,\n","                                        padding=1, bias=use_bias)\n","            down = [downrelu, downconv]\n","            up = [uprelu, upconv, upnorm]\n","            model = down + up\n","        else:\n","            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n","                                        kernel_size=4, stride=2,\n","                                        padding=1, bias=use_bias)\n","            down = [downrelu, downconv, downnorm]\n","            up = [uprelu, upconv, upnorm]\n","\n","            if use_dropout:\n","                model = down + [submodule] + up + [nn.Dropout(0.5)]\n","            else:\n","                model = down + [submodule] + up\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        if self.outermost:\n","            return self.model(x)\n","        else:   # add skip connections\n","            return torch.cat([x, self.model(x)], 1)\n","    \n","\n","    \n","class UnetGenerator(nn.Module):\n","    \"\"\"Create a Unet-based generator\"\"\"\n","\n","    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n","        \"\"\"Construct a Unet generator\n","        Parameters:\n","            input_nc (int)  -- the number of channels in input images\n","            output_nc (int) -- the number of channels in output images\n","            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n","                                image of size 128x128 will become of size 1x1 # at the bottleneck\n","            ngf (int)       -- the number of filters in the last conv layer\n","            norm_layer      -- normalization layer\n","\n","        We construct the U-Net from the innermost layer to the outermost layer.\n","        It is a recursive process.\n","        \"\"\"\n","        super(UnetGenerator, self).__init__()\n","        # construct unet structure\n","        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, \n","                                             norm_layer=norm_layer, innermost=True)  # add the innermost layer\n","        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n","            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, \n","                                                 norm_layer=norm_layer, use_dropout=use_dropout)\n","        # gradually reduce the number of filters from ngf * 8 to ngf\n","        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, \n","                                             norm_layer=norm_layer)\n","        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, \n","                                             norm_layer=norm_layer)\n","        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, \n","                                             norm_layer=norm_layer)\n","        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, \n","                                             outermost=True, norm_layer=norm_layer)  # add the outermost layer\n","\n","    def forward(self, input):\n","        \"\"\"Standard forward\"\"\"\n","        return self.model(input)\n","\n","    \n","def define_G(input_nc, output_nc, ngf, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\n","    net = None\n","    norm_layer = get_norm_layer(norm_type=norm)\n","\n","#     net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n","    net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n","    return init_net(net, init_type, init_gain, gpu_id)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gc1AGWUiM4NU","colab_type":"text"},"source":["### **Discriminator**"]},{"cell_type":"code","metadata":{"id":"h3WTq1hpM4NV","colab_type":"code","colab":{}},"source":["class NLayerDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n","        super(NLayerDiscriminator, self).__init__()\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        kw = 4\n","        padw = 1\n","        sequence = [\n","            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        nf_mult = 1\n","        nf_mult_prev = 1\n","        for n in range(1, n_layers):\n","            nf_mult_prev = nf_mult\n","            nf_mult = min(2**n, 8)\n","            sequence += [\n","                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n","                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n","                norm_layer(ndf * nf_mult),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        nf_mult_prev = nf_mult\n","        nf_mult = min(2**n_layers, 8)\n","        sequence += [\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n","                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n","\n","        if use_sigmoid:\n","            sequence += [nn.Sigmoid()]\n","\n","        self.model = nn.Sequential(*sequence)\n","\n","    def forward(self, input):\n","        return self.model(input)\n","\n","\n","class PixelDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n","        super(PixelDiscriminator, self).__init__()\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        self.net = [\n","            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n","            nn.LeakyReLU(0.2, True),\n","            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n","            norm_layer(ndf * 2),\n","            nn.LeakyReLU(0.2, True),\n","            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n","\n","        if use_sigmoid:\n","            self.net.append(nn.Sigmoid())\n","\n","        self.net = nn.Sequential(*self.net)\n","\n","    def forward(self, input):\n","        return self.net(input)\n","    \n","def define_D(input_nc, ndf, netD,\n","             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\n","    net = None\n","    norm_layer = get_norm_layer(norm_type=norm)\n","\n","    if netD == 'basic':\n","        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n","    elif netD == 'n_layers':\n","        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n","    elif netD == 'pixel':\n","        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n","    else:\n","        raise NotImplementedError('Discriminator model name [%s] is not recognized' % net)\n","\n","    return init_net(net, init_type, init_gain, gpu_id)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EnXYTrODM4NX","colab_type":"text"},"source":["### **init_param, loss fn, optimizer**"]},{"cell_type":"code","metadata":{"id":"eKlPjQNCM4NY","colab_type":"code","colab":{}},"source":["def init_weights(net, init_type='normal', gain=0.02):\n","    def init_func(m):\n","        classname = m.__class__.__name__\n","        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n","            if init_type == 'normal':\n","                init.normal_(m.weight.data, 0.0, gain)\n","            elif init_type == 'xavier':\n","                init.xavier_normal_(m.weight.data, gain=gain)\n","            elif init_type == 'kaiming':\n","                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","            elif init_type == 'orthogonal':\n","                init.orthogonal_(m.weight.data, gain=gain)\n","            else:\n","                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n","            if hasattr(m, 'bias') and m.bias is not None:\n","                init.constant_(m.bias.data, 0.0)\n","        elif classname.find('BatchNorm2d') != -1:\n","            init.normal_(m.weight.data, 1.0, gain)\n","            init.constant_(m.bias.data, 0.0)\n","\n","    print('initialize network with %s' % init_type)\n","    net.apply(init_func)\n","\n","\n","\n","def get_norm_layer(norm_type='instance'):\n","    if norm_type == 'batch':\n","        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n","    elif norm_type == 'instance':\n","        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n","    elif norm_type == 'switchable':\n","        norm_layer = SwitchNorm2d\n","    elif norm_type == 'none':\n","        norm_layer = None\n","    else:\n","        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n","    return norm_layer\n","\n","\n","class GANLoss(nn.Module):\n","    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n","        super(GANLoss, self).__init__()\n","        self.register_buffer('real_label', torch.tensor(target_real_label))\n","        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n","        if use_lsgan:\n","            self.loss = nn.MSELoss()\n","        else:\n","            # binary cross entropy\n","            self.loss = nn.BCELoss()\n","\n","    def get_target_tensor(self, input, target_is_real):\n","        if target_is_real:\n","            target_tensor = self.real_label\n","        else:\n","            target_tensor = self.fake_label\n","        return target_tensor.expand_as(input)\n","\n","    def __call__(self, input, target_is_real):\n","        target_tensor = self.get_target_tensor(input, target_is_real)\n","        return self.loss(input, target_tensor)\n","    \n","def update_learning_rate(scheduler, optimizer):\n","    scheduler.step()\n","    lr = optimizer.param_groups[0]['lr']\n","    print('learning rate = %.7f' % lr)  \n","\n","# learning rate decay\n","def get_scheduler(optimizer):\n","    if lr_policy == 'lambda':\n","        def lambda_rule(epoch):\n","            lr_l = 1.0 - max(0, epoch + epoch_count - niter) / float(niter_decay + 1)\n","            return lr_l\n","        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n","    elif lr_policy == 'step':\n","        scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_decay_iters, gamma=0.1)\n","    elif lr_policy == 'plateau':\n","        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n","    elif lr_policy == 'cosine':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\n","    else:\n","        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n","    return scheduler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uysr5ST9M4Na","colab_type":"text"},"source":["### **train**"]},{"cell_type":"code","metadata":{"id":"SZ1djfz4M4Nb","colab_type":"code","colab":{}},"source":["dataroot1 = \"datasets/facades/train\"\n","dataroot2 = \"datasets/facades/test\"\n","# As for windows, delete num_works parameter\n","training_data_loader = DataLoader(dataset=DatasetFromFolder(dataroot1, direction), num_workers=threads, batch_size=batch_size, shuffle=True)\n","testing_data_loader = DataLoader(dataset=DatasetFromFolder(dataroot2, direction), num_workers=threads, batch_size=test_batch_size)\n","\n","\n","print('===> Building models')\n","\n","'''loading the generator and discriminator'''\n","net_g = define_G(input_nc, output_nc, ngf, 'batch', False, 'normal', 0.02, gpu_id=device)\n","net_d = define_D(input_nc + output_nc, ndf, 'basic', gpu_id=device)\n","\n","'''set loss fn'''\n","criterionGAN = GANLoss().to(device)\n","criterionL1 = nn.L1Loss().to(device)\n","criterionMSE = nn.MSELoss().to(device)\n","\n","'''setup optimizer''' \n","optimizer_g = optim.Adam(net_g.parameters(), lr=lr, betas=(beta1, 0.999))\n","optimizer_d = optim.Adam(net_d.parameters(), lr=lr, betas=(beta1, 0.999))\n","\n","'''set the learning rate adjust policy'''\n","net_g_scheduler = get_scheduler(optimizer_g)\n","net_d_scheduler = get_scheduler(optimizer_d)\n","\n","'''training process'''\n","for epoch in range(epoch_count, niter + niter_decay + 1):\n","    \n","    for iteration, batch in enumerate(training_data_loader, 1):\n","        # forward\n","        real_a, real_b = batch[0].to(device), batch[1].to(device)\n","        fake_b = net_g(real_a)\n","\n","        ######################\n","        # (1) Update D network\n","        ######################\n","\n","        optimizer_d.zero_grad()\n","        \n","        # D train with fake\n","        fake_ab = torch.cat((real_a, fake_b), 1)\n","        pred_fake = net_d.forward(fake_ab.detach())\n","        loss_d_fake = criterionGAN(pred_fake, False)\n","\n","        # D train with real\n","        real_ab = torch.cat((real_a, real_b), 1)\n","        pred_real = net_d.forward(real_ab)\n","        loss_d_real = criterionGAN(pred_real, True)\n","        \n","        # Combined D loss\n","        loss_d = (loss_d_fake + loss_d_real) * 0.5\n","\n","        loss_d.backward()\n","       \n","        optimizer_d.step()\n","\n","        ######################\n","        # (2) Update G network\n","        ######################\n","\n","        optimizer_g.zero_grad()\n","\n","        # First, G(A) should fake the discriminator\n","        fake_ab = torch.cat((real_a, fake_b), 1)\n","        pred_fake = net_d.forward(fake_ab)\n","        loss_g_gan = criterionGAN(pred_fake, True)\n","\n","        # Second, G(A) = B\n","        loss_g_l1 = criterionL1(fake_b, real_b) * lamb\n","        \n","        loss_g = loss_g_gan + loss_g_l1\n","        \n","        loss_g.backward()\n","\n","        optimizer_g.step()\n","\n","        print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\n","            epoch, iteration, len(training_data_loader), loss_d.item(), loss_g.item()))\n","\n","    update_learning_rate(net_g_scheduler, optimizer_g)\n","    update_learning_rate(net_d_scheduler, optimizer_d)\n","\n","    # test\n","    avg_psnr = 0\n","    for batch in testing_data_loader:\n","        input, target = batch[0].to(device), batch[1].to(device)\n","\n","        prediction = net_g(input)\n","        mse = criterionMSE(prediction, target)\n","        psnr = 10 * log10(1 / mse.item())\n","        avg_psnr += psnr\n","    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))\n","\n","    #checkpoint\n","    if epoch % 50 == 0:\n","        if not os.path.exists(\"checkpoint\"):\n","            os.mkdir(\"checkpoint\")\n","        if not os.path.exists(os.path.join(\"checkpoint\", dataset)):\n","            os.mkdir(os.path.join(\"checkpoint\", dataset))\n","        net_g_model_out_path = \"checkpoint/{}/netG_model_epoch_{}.pth\".format(dataset, epoch)\n","        net_d_model_out_path = \"checkpoint/{}/netD_model_epoch_{}.pth\".format(dataset, epoch)\n","        torch.save(net_g, net_g_model_out_path)\n","        torch.save(net_d, net_d_model_out_path)\n","        print(\"Checkpoint saved to {}\".format(\"checkpoint\" + dataset))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8yzlAg1CM4Ne","colab_type":"text"},"source":["### **Test**"]},{"cell_type":"code","metadata":{"id":"Xu3C-qNuM4Nf","colab_type":"code","outputId":"78805bca-8030-460a-ea60-0e4184411d9e","colab":{}},"source":["nepochs = 150\n","model_path = \"checkpoint/{}/netG_model_epoch_{}.pth\".format(dataset, nepochs)\n","\n","net_g = torch.load(model_path).to(device)\n","\n","if direction == \"a2b\":\n","    image_dir = \"datasets/{}/test/a/\".format(dataset)\n","else:\n","    image_dir = \"datasets/{}/test/b/\".format(dataset)\n","\n","image_filenames = [x for x in os.listdir(image_dir) if is_image_file(x)]\n","\n","transform_list = [transforms.ToTensor(),\n","                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n","\n","transform = transforms.Compose(transform_list)\n","\n","for image_name in image_filenames:\n","    img = load_img(image_dir + image_name)\n","    img = transform(img)\n","    input = img.unsqueeze(0).to(device)\n","    out = net_g(input)\n","    out_img = out.detach().squeeze(0).cpu()\n","\n","    if not os.path.exists(os.path.join(\"result\", dataset)):\n","        os.makedirs(os.path.join(\"result\", dataset))\n","    save_img(out_img, \"result/{}/{}\".format(dataset, image_name))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'dataset' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-12-be723b2f1915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mnepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"checkpoint/{}/netG_model_epoch_{}.pth\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mnet_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"]}]}]}